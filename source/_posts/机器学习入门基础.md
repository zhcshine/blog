---
title: 机器学习入门基础
date: 2019-11-06 12:57:54
tags:
---

# 概念

### 机器学习方式

> 监督式学习: 给一组数据一个对应的输出结果。从数据中挖掘相关性

> 非监督式学习: 通过预先引入的优化准则进行模型训练

> 强化学习/半监督式学习

### 机器学习目标

> 分类(监督式学习)

> 根据数据集目标的特征或者属性，划分到已有的分类中

# 算法

### 常用算法

> K近邻(KNN), 逻辑回归， 决策树， 朴素贝叶斯

### KNN算法

```python
# -*- coding: utf-8 -*-
# https://www.imooc.com/video/20172
# 图像展示
import matplotlib.pyplot as plt
from sklearn import datasets
# 调用knn模型
from sklearn.neighbors import KNeighborsClassifier
# 引入准确率
from sklearn.metrics import accuracy_score
# 引入数据分离
from sklearn.model_selection import train_test_split

def load_data():
    # 区分属性数据和结果数据
    # 加载iris数据
    iris = datasets.load_iris()
    # 属性数据
    # print(iris.data)
    # 属性数据含义
    # print(iris.feature_names)
    # 结果数据
    # print(iris.target)
    # 结果数据含义
    # print(iris.target_names)
    # 确认属性/结果数据类型
    # print(type(iris.data), type(iris.target))
    # 确认行数一致
    # print(iris.data.shape, iris.target.shape)
    return iris.data, iris.target


# 预测
def knn_predict(data):
    # X输入数据赋值 y输出数据赋值
    x = data[0]
    y = data[1]
    # knn临近值的设置
    k = 5
    # 创建实例
    knn = KNeighborsClassifier(n_neighbors=k)
    # 模型训练
    knn.fit(X=x, y=y)
    # 开始预测
    x_test = [[1, 2, 3, 4], [2, 3, 4, 5], [2, 4, 1, 2]]
    results = knn.predict(x_test)
    print(results)


def knn_pred(data):
    x = data[0]
    y = data[1]
    # knn临近值的设置
    k = 5
    # 创建实例
    knn = KNeighborsClassifier(n_neighbors=k)
    # 模型训练
    knn.fit(X=x, y=y)
    y_pred = knn.predict(x)
    print(y_pred)
    print(y_pred.shape)
    # 准确率计算
    print(accuracy_score(y, y_pred))


# 训练集与预测集 计算准确率
def knn_test_data(data):
    # X输入数据赋值 y输出数据赋值
    x = data[0]
    y = data[1]
    print(x.shape)
    print(y.shape)
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)
    print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
    k = 5
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x_train, y_train)
    # 训练集预测
    y_train_pred = knn.predict(x_train)
    print(accuracy_score(y_train, y_train_pred))
    # 测试集预测
    y_test_pred = knn.predict(x_test)
    print(accuracy_score(y_test, y_test_pred))


# 获得最高准确率的k值
def knn_n_neighbors(data):
    # X输入数据赋值 y输出数据赋值
    x = data[0]
    y = data[1]
    k_range = list(range(1, 26))
    score_train = []
    score_test = []
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(x_train, y_train)
        # 训练集预测
        y_train_pred = knn.predict(x_train)
        score_train.append(accuracy_score(y_train, y_train_pred))
        # 测试集预测
        y_test_pred = knn.predict(x_test)
        score_test.append(accuracy_score(y_test, y_test_pred))
    # 展示k值与训练集测试集准确率关系
    # plt.plot(k_range, score_train)
    # plt.xlabel('K(KNN MODEL)')
    # plt.ylabel('Training Accuracy')
    # plt.show()
    plt.plot(k_range, score_test)
    plt.xlabel('K(KNN MODEL)')
    plt.ylabel('Test Accuracy')
    plt.show()


def index():
    data = load_data()
    # knn_predict(data)
    # knn_pred(data)
    # knn_test_data(data)
    knn_n_neighbors(data)

if __name__ == '__main__':
    index()

```

### 逻辑回归算法

> 用于解决分类问题的一种模型，计算其归属于某一类别的概率P(X), 常用与二分类问题，例如是否是垃圾邮件，是否是猫等等

```python

# -*- coding: utf-8 -*-
import pandas as pd
from sklearn.model_selection import train_test_split
# 逻辑回归模型
from sklearn.linear_model import LogisticRegression
from sklearn import metrics


def index():
    path = 'diabetes.csv'
    pima = pd.read_csv(path)
    print(pima.head())
    # 怀孕次数， 胰岛素水平， 体重指数， 年龄
    feature_names = ['Pregnancies', 'Insulin', 'BMI', 'Age']
    # x, y赋值
    x = pima[feature_names]
    y = pima.Outcome
    # 维度确认
    print(x.shape)
    print(y.shape)
    # 数据分离  random_state=0表示每次都是相同的分离
    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)
    # 创建模型实例
    logeg = LogisticRegression()
    logeg.fit(x_train, y_train)
    # 基于测试数据集的预测
    y_pred = logeg.predict(x_test)
    # 使用准确率进行评估
    print(metrics.accuracy_score(y_test, y_pred))
    # 确认正负样本数据量
    print(y_test.value_counts())
    # 1的比例, 0的比例
    print(y_test.mean(), 1-y_test.mean())
    # 空准确率
    print(max(y_test.mean(), 1-y_test.mean()))
    # 以上发现训练后的模型准确率基本等同与空准确率
    # 引入混淆矩阵
    # 计算混淆矩阵
    confusion = metrics.confusion_matrix(y_test, y_pred)
    print(confusion)
    # 展示部分实际结果与预测结果(25组)
    print("true:", y_test.values[0:25])
    print("pred:", y_pred[0:25])
    # 四个因子赋值
    tn = confusion[0][0]  # True Negatives 预测准备 实际为负样本的数量
    fp = confusion[0][1]  # False Positives 预测错误 实际为父样本的数量
    fn = confusion[1][0]  # False Negatives 预测错误 实际为正样本的数量
    tp = confusion[1][1]  # True Positives 预测准确 实际为正样本的数量
    print(tn, fp, fn, tp)
    print('准确率Accuracy: 所有样本中，预测正确的比例')
    print((tp + tn) / (tp + tn + fp + fn))
    print('错误率Misclassification Rate： 所有样本中，预测错误的比例')
    print((fp + fn) / (tp + tn + fp + fn))
    print('灵敏度(召回率)Sensitivity： 正样本中，预测正确的比例')
    recall = tp / (tp + fn)
    print(recall)
    print('特异性Specificity: 负样本中，预测正确的比例')
    print(tn / (tn + fp))
    print('精确率Precision: 预测结果为正样本中， 预测正确的比例')
    precision = tp / (tp + fp)
    print(precision)
    print('F1分数Score: 综合判断指标')
    print((2*precision*recall) / (precision+recall))


if __name__ == '__main__':
    index()

```
